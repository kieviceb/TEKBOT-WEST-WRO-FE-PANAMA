# TEKBOT WEST's readme <img src="https://upload.wikimedia.org/wikipedia/commons/a/ab/Flag_of_Panama.svg" alt="Bandera de Panamá" width="30"/>
<p align="center">
  <img src="https://github.com/user-attachments/assets/516d8dd3-5f5f-4bb0-b4f9-02e0fee0c09e" alt="TEKBOT (1)">
</p>


[![Instagram](https://img.shields.io/badge/Instagram-%23E9805F.svg?style=for-the-badge&logo=Instagram&logoColor=white)](https://www.instagram.com/tekbot_lab?utm_source=ig_web_button_share_sheet&igsh=ZDNlZDc0MzIxNw==)
[![Facebook](https://img.shields.io/badge/YouTube-%23E4445F.svg?style=for-the-badge&logo=Youtube&logoColor=white)](https://www.youtube.com/@TEKBOT_LAB)


> [!NOTE]
> This github repository contains many illustrative content such as images,
> gifs and video, so wait till is fully refresh to see it in its entirety.  
> 
# 🐙 Repo's folder structure/overview
```
📦 TEKBOT-WEST-WRO-FE
├── 📁 3D_printables # 3D models ready for printing robot components
├── 📁 schemes # Wiring and schematic diagram
├── 📁 src # Source code for robot control and challenge algorithms
├── 📁 team photos # Photos of the team 
├── 📁 vehicle photos # Pictures of the robot at different views
├── 📁 video # Video of testing competition runs
├── 📄 README.md # Main project documentation
```
---

<img width="1000" height="520" alt="AHHHHH_processed_by_imagy" src="https://github.com/user-attachments/assets/4d39ecf9-c464-4f2b-9efc-c3b949ed69be" />
<a href="https://github.com/kieviceb/TEKBOT-WEST-WRO-FE-PANAMA/tree/main/team%20photos">
  <img width="1000" height="2000" alt="BEHIND THE SCREENS (2)" src="https://github.com/user-attachments/assets/34d80203-9c0d-4e8b-a392-d2f8124f5f28" />
</a>


---
# Vehicle Photos 📸
<p align="center">
  <img src="https://github.com/user-attachments/assets/cb52b6d4-e43e-412f-b74b-98dca438176e" alt="TEKBOT (1)">
</p>

<img width="1000" height="1917" alt="Copia de BEHIND THE SCREENS" src="https://github.com/user-attachments/assets/b97cae86-9007-4df4-bbb8-74799222fe50" />


# Components 🧱
A list of all the electrical and mechanical components on the robot.

| <img src="https://github.com/user-attachments/assets/719a51d8-4b14-402d-a462-ba1e4b071c2c" alt="Alt 1" width="200"/> | <img src="https://github.com/user-attachments/assets/3641b928-34c5-4d97-861c-fa08d40c9faa" alt="Alt 1" width="200"/> | <img src="https://github.com/user-attachments/assets/3d5651d3-c8fe-4935-9561-6ca9d9c87a76" alt="Alt 1" width="200"/> | 
| :------------: |:-------------:| :------------:|
|[RASPBERRY PI 5 - 8GB RAM x1](https://a.co/d/7e9fzE2)|[MICROSOFT LIFECAM HD-3000 x1](https://a.co/d/66cNAOh)|[L298N MOTOR DRIVER x1](https://www.steren.com.pa/tarjeta-para-control-de-motores-cc-l298n.html)|
| <img src="https://github.com/user-attachments/assets/5b0ffc5d-ce02-4620-9849-15fdce566702" width="200"/> | <img src="https://github.com/user-attachments/assets/d4170adc-23b9-446f-bac0-1c50b966e00f" alt="Alt 1" width="200"/> | <img src="https://github.com/user-attachments/assets/db07c349-4f9f-4661-8327-3c710a5d41c6" alt="Alt 1" width="200"/> |
|[ARDUINO NANO RP2040 CONNECT x1](https://a.co/d/hI8sZ40) |[FUNDUINO KIT CHASSIS x1](https://a.co/d/fpJSHg1)|[GEEKWORM X1202 4-Cell 5V UPS Shiled+Active Cooler H505 x1](https://a.co/d/hs9rgxH) |
| <img src="https://github.com/user-attachments/assets/8002e73e-910a-462e-b991-5cd5c858e316" width="200"/> |<img src="https://github.com/user-attachments/assets/77157511-58ff-4ee8-9977-d3b53d906af6" alt="Alt 1" width="200"/>| <img src="https://github.com/user-attachments/assets/86d8bf70-708b-426f-896a-9283a74d13df" alt="Alt 1" width="200"/> |
|[JUMPER WIRES](https://a.co/d/cw9IdJk)|[MG996R METAL GEAR SERVO x1](https://a.co/d/cRVAc0u)|[STEREN Li-ion 2800 mAh RRECHARGABLE BATTERY x7](https://www.steren.com.pa/bateria-recargable-li-ion-2800-mah-tipo-18650-1.html)|
| <img src="https://github.com/user-attachments/assets/ff93515e-6ec1-41f5-be4c-ac25b1e24010" width="200"/> |<img src="https://github.com/user-attachments/assets/44a5bca6-2fb6-49b7-8374-93a03705a72b" alt="Alt 1" width="200"/>| <img src="https://github.com/user-attachments/assets/33ee1aaf-4750-4b80-978a-aab5d8d9adeb" alt="Alt 1" width="200"/> |
|[ULTRASONIC DISTANCE SENSOR x3](https://a.co/d/cxbABTR)|[LEGO EV3 DC MOTOR](https://ebay.us/m/6vxrYN)|[COOLING FAN x3](https://a.co/d/frwMkoA)|
| <img src="https://github.com/user-attachments/assets/c4632267-e417-4f33-bfa0-a858d9f1e28e" width="200"/> |<img src="https://github.com/user-attachments/assets/dd5c7b13-b8ad-4aa0-9481-8411c9bf2b1a" alt="Alt 1" width="200"/>| <img src="https://github.com/user-attachments/assets/72f660f8-0134-4bb3-aa97-c8b0ae3c5f5a" alt="Alt 1" width="200"/> |
|[MINI BREADBOARD x1](https://a.co/d/0logMX9)|[PUSH BUTTON x1](https://a.co/d/70cfCgl)|[10K OHM RESISTOR x1](https://a.co/d/dTNRWBD)|

THE MAGIC
| :------------: |
| <img src="https://github.com/user-attachments/assets/be93e47a-3ca9-4626-8b98-f7ea953e2264" alt="Alt 1" width="200"/> |
|[GOOGLE CORAL USB EDGE TPU ACCELARATOR x1](https://a.co/d/eEklwQk)|

# ⚡Circuit diagram

<img width="1920" height="1080" alt="Wiring diagram" src="https://github.com/user-attachments/assets/a8a37846-0950-4cb8-8dc3-2045f952b8c9" />

# Future Engineers Challenge Overview

The **WRO 2025 Future Engineers challenge** challenges teams to design and build a fully autonomous robot capable of navigating a dynamic and randomized racetrack. Using a combination of sensors, computer vision, and advanced control algorithms, the robot must adapt to changing obstacles, follow predefined driving rules, and demonstrate precise maneuvering skills.

### 📌 Competition Format

- **🏁 Open Challenge:**  
  The robot must complete **three (3) laps** around a track where obstacles and track elements are randomly placed, requiring real-time adaptation.

- **🚦 Obstacle Challenge:**  
  The robot must detect and respond correctly to randomly positioned colored markers:  
  - 🟥 **Red signs** → Robot must drive on the right side of the lane.  
  - 🟩 **Green signs** → Robot must drive on the left side of the lane.

  After completing the laps, the robot must find a designated parking zone and perform a precise **parallel parking maneuver** within a confined space, adding an additional technical challenge.

### 📑 Documentation Requirements

Each team is required to maintain a **public GitHub repository** documenting their engineering design process, technical decisions, robot design, and source code. This encourages transparency, collaboration, and learning within the robotics community.

### 🏆 Scoring & Evaluation

Teams are evaluated based on their robot’s **accuracy**, **speed**, and the quality of their **technical documentation**. Points are awarded to teams that effectively balance performance, adaptability to randomized conditions, and innovation in their approach. This challenge promotes not only technical skills in robotics and programming but also problem-solving, teamwork, and creative engineering.
> [!WARNING]
>
> We noticed a translation misunderstanding regarding the rules, at the original english version it says
> that the robot starting within the parking lot is 7 points, but in the spanish version states that the robot should be on
> In the parking lot, not specifying if before or after the challenge is done, but taking as reference the original english version
> the robot starting on the parking lot can score 7 points.
>
<img width="653" height="186" alt="meet the team (3)" src="https://github.com/user-attachments/assets/3d4dfe50-6bb6-4aca-be4a-6e9f3429dde8" />

---

> [!NOTE]
> For detailed rules and regulations (PANAMA ONLY), please refer to the official WRO Panama 2025 Future Engineers document:  
> [WRO 2025 Future Engineers Rules (PDF)](https://fundesteam.nyc3.cdn.digitaloceanspaces.com/FutureEngineers/WRO-2025-FE-Reglas%20Generales.pdf)

<br>


# ⚙️ Mobility Management

### Movement
- **LEGO EV3 DC Motor** driven by an L298N H-bridge on the Arduino Nano RP2040, powered by a 3×18650 Li-ion pack (≈11.1 V) for high torque.
- PWM speed control (pin D3) and direction pins (D4/D5), with automatic halt if HC-SR04 front distance < threshold.
- Receives speed & direction commands over Serial1 from the Raspberry Pi.

### Steering
- **MG996R metal-gear servo** in an Ackermann linkage for accurate, repeatable turns (pin D2).
- Servo angles (20° left, 90° center, 155° right) sent as ASCII strings over Serial1 and output via PWM.

# 🔋 Power Management

Our robot uses a streamlined power system for stable, efficient energy distribution:

- **Geekworm UPS Module**  
  All computing units (Raspberry Pi & Arduino Nano RP2040) are powered directly from the Geekworm Raspberry Pi Wide Input Voltage UPS, providing regulated 5 V output and battery backup—no external power bank needed.

- **Servo Power**  
  The MG996R steering servo draws its 5 V supply from the UPS module, ensuring stable, synchronized operation.

- **Camera and CORAL accelerator Power**  
  The Microsoft LifeCam HD-3000 and the CORAL connect via USB to the Raspberry Pi and is powered through the UPS, eliminating separate power sources.

- **DC Motor Power**  
  The L298N H-bridge and DC motor are driven by a dedicated pack of three 18650 Li-ion cells in series (≈11.1 V), delivering the higher voltage and current the motor requires.

This setup guarantees reliable power under all operating conditions.  


# 📡 Robot Communication

Our robot implements a structured communication system between its two core processing units to ensure synchronized and reliable operation:

- The **Raspberry Pi** (SBC – Single Board Computer) acts as the **master**. It is responsible for high-level tasks such as image processing, object detection, strategy decision-making, and general system coordination.

- The **Arduino Nano RP2040** (SBM – Secondary Board Microcontroller) acts as the **slave**. It is dedicated to real-time control of low-level hardware components, such as driving the servo motor, ultrasonic sensors, LEGO EV3 DC MOTOR and interpreting incoming instructions from the master.

### 🔁 Serial Communication

The two boards communicate via a **UART (Serial) interface** using the following configuration:

- **Raspberry Pi TX (GPIO 14)** → **Arduino RX (pin 1)**  
- **Raspberry Pi RX (GPIO 15)** ← **Arduino TX (pin 0)**

This full-duplex serial link enables:

- The **Raspberry Pi (master)** to send instructions like servo angles or movement triggers based on camera input.
- The **Arduino (slave)** to receive commands and execute precise control over hardware.
- **Optional feedback**: The Arduino can also send status messages or sensor feedback back to the Raspberry Pi when necessary.

This master-slave architecture promotes:

- **Modularity**, by separating high-level logic and low-level control.
- **Responsiveness**, with real-time actions handled by the microcontroller.
- **Scalability**, allowing for future hardware expansions without major rewrites.

The system is optimized for efficiency and robustness in fast-paced robotic challenges.

# 👁️ Sense and Object Detection

Our robot relies on a combination of vision (OpenCV + ML on Coral Accelerator) and ultrasonic sensing to understand its environment and make driving decisions.

## Vision System

<p align="center">
  <img src="https://github.com/user-attachments/assets/5609bb93-1abd-4938-874a-78750336fe9e" alt="Dataset samples" width="350"/>
</p>

- We use the Microsoft LifeCam HD-3000 connected to the Raspberry Pi. The Pi processes the camera feed with OpenCV and a Machine Learning model accelerated by Coral to:
- Detect red and green blocks for obstacle avoidance.
- Recognize the magenta parking spot to initiate the parking routine.
- Detect black walls and estimate their distance.
- Send steering and speed commands to the Arduino Nano RP2040 over Serial.

## Ultrasonic Distance Sensing

<p align="center">
  <img src="https://github.com/user-attachments/assets/6b70fd23-10f5-44be-91a9-fc63dd27e047" alt="Dataset samples" width="350"/>
</p>

- The Arduino Nano RP2040 reads three HC-SR04 ultrasonic sensors (left, center, right) to:
    - Measure the robot’s distance from the walls.
    - Detect obstacles that may not be visible to the camera.
    - Provide redundancy and safety in case of vision failure.
    - This multi-sensor approach ensures robust performance:
    - Vision + ML handles colored block detection, parking, and wall recognition.
    - Ultrasonics provide accurate distance measurements for safety and redundancy.
      
## Machine Learning  

To improve robustness, we trained a **custom object detection model** specifically for our robot’s environment.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/6a6bf5d5-deef-4753-a1c9-7cb3e5085e32" alt="Dataset samples" width="400"/>
</p>

- The dataset was built using **Roboflow** 👉 our data set [here](https://universe.roboflow.com/tekbot-west/tekbot-west-nacional-bbbm9/dataset/2).  
- We collected and labeled images of **green/red blocks, walls, and parking spots** at different **distances, angles, and lighting conditions**, to ensure stable detection during competition with minimal calibration.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/8c8a763d-c597-4432-adf1-6ebd00906060" alt="YOLOv8 Training" width="400"/>
</p>

- The model uses the **YOLOv8 format**, which is well-suited for real-time detection thanks to its **lightweight architecture and speed** — ideal for embedded systems.  
- Training was done in **Google Colab** using GPU acceleration.  
- Finally, we exported the trained model to **TensorFlow Lite**, optimized for the **Coral Edge TPU accelerator** on the Raspberry Pi.  

This pipeline allows the robot to run **real-time detection with low latency**, ensuring reliable navigation and obstacle avoidance.  

# ⚔️ Challenge Strategies
## Open Challenge Strategy <img src="https://github.com/user-attachments/assets/407b5824-f865-4c18-9c02-1df7553b5fca" alt="Bandera de Panamá" width="40"/>
- **Start Command**  
  Wait for `'s'` on Serial1 before beginning navigation.

- **Ultrasonic Sensing**  
  Continuously read left, center and right HC-SR04 sensors for obstacle distances.

- **Front Obstacle Handling**  
  If center distance ≤ FRONT_MIN, halt motor; otherwise drive at fixed PWM speed.

- **Emergency Side Avoidance**  
  If left or right distance ≤ SIDE_MIN, immediately steer hard away (servo to 150° or 20°).

- **PD Steering Control**  
  When no emergency, compute error = dR − dL and derivative, apply Kp/Kd to adjust servo within +60° -70° o deadzone around 90°.  
  This ensures smooth alignment between the two side walls.

- **Lap Counting via Encoder**  
  The IMU is no longer used. Instead, we rely on the wheel encoder:  
  - Calibration showed that **10,000 encoder counts ≈ 30 cm of travel**.  
  - Since each lap around the track is ~2 m, that corresponds to ~66,667 encoder counts.  
  - Running multiple trials from different start positions, we computed a **median value** for 3 laps.  
  - The robot stops when the encoder reading reaches this median threshold, ensuring reliable lap counting independent of curves.

- **Completion Condition**  
  Stop motor and center servo once the encoder threshold for 3 laps is reached; remain halted.

- **Serial Feedback**  
  Print encoder values, lap progress, and status messages over Serial1 for real-time monitoring.

## Obstacle Challenge Strategy <img src="https://github.com/user-attachments/assets/e833c70a-b1fc-4165-ae0d-35a571c72e85" alt="Bandera de Panamá" width="40"/>

- **Start & Exit Detection**  
  At the beginning, the robot uses **ultrasonic sensors** to measure wall distances and determine **which side it exits from the parking area**.  
  This defines its initial orientation on the track.

- **Machine Learning Object Detection**  
  Instead of simple LAB masking, we use a **YOLOv8-based model** (dataset and labels in Roboflow, trained in Google Colab and exported to TensorFlow Lite, and accelerated with Coral).  
  The model detects **red blocks, green blocks, black walls, and the magenta parking zone** in real time.  

- **Block Evasion Strategy**  
  When a block is detected:  
  - **Red →** compute a temporary target position **to the right side** and steer the robot to bypass the block.  
  - **Green →** compute a temporary target position **to the left side** for evasion.  
  The robot continues toward this evasion target until the block is cleared, then smoothly returns to its central trajectory.  

- **Lap Counting via Parking Detection**  
  Instead of counting curves or encoder ticks, the robot uses **ML detection of the magenta parking spot** as a lap marker.  
  - Each time the robot passes the parking, the counter increases by **1 lap**.  
  - After **3 passes (3 laps)**, the robot transitions to the final parking stage.  

- **PD Steering Control**  
  During normal navigation, the robot still uses **PD control** based on ultrasonic side distances (`dR - dL`) to stay centered in the lane, ensuring smooth driving even without visible blocks.  

- **Final Parking Routine**  
  Once 3 laps are completed, the robot searches for the **magenta parking zone** using ML detection.  
  Upon recognition, it executes a **parallel parking maneuver**, aligning itself precisely inside the designated parking spot.  

- **Serial Communication**  
  All steering (`S<angle>`) and motor commands are sent from the Raspberry Pi to the Arduino Nano via Serial, ensuring synchronized control.


# `</>` Into the codes (code explanations)
# `</>` Into the codes (code explanations)

## 1. **Open Challenge Code — Main loop (core)**
> To keep this README concise, here we only explain the **main loop** (the core logic).  
> A complete explanation and all helper functions can be found in the [`/src`](./src) folder.  

```cpp
void loop() {

  if (!started) {
    if (Serial1.available() && Serial1.read() == 's') {
      started = true;
      Serial1.println("Starting navigation…");
      prevTime = micros();       // reset PD timing
      prevError = 0;
    } else {
      return;                    // stay idle until 's' is received
    }
  }
```
-  Wait for the start command over Serial1 ('s')
```cpp
 
  float dL = readUltrasonic(TRIG_LEFT,  ECHO_LEFT);   // left wall/obstacle
  float dC = readUltrasonic(TRIG_CENT,  ECHO_CENT);   // front obstacle
  float dR = readUltrasonic(TRIG_RIGHT, ECHO_RIGHT);  // right wall/obstacle

  // C) Front obstacle handling: stop if too close, otherwise run motor at fixed PWM
  if (dC <= FRONT_MIN) {
    analogWrite(ENB, 0);                 // stop motor
  } else {
    analogWrite(ENB, MOTOR_SPEED);       // forward at constant speed
  }
```
- Read distances (cm) from the 3 HC-SR04 ultrasonic sensors
```cpp
  if (dL <= SIDE_MIN_LEFT) {
    steeringServo.write(SERVO_MAX_RIGHT);  // steer right
  }
  else if (dR <= SIDE_MIN_RIGHT) {
    steeringServo.write(SERVO_MAX_LEFT);   // steer left
  }
  else {
```
 - Emergency side avoidance: steer hard away if a wall is too close
```cpp
    unsigned long now = micros();
    float dt = (now - prevTime) / 1e6;     // seconds
    prevTime = now;

    float error = dR - dL;                  // if dR>dL → robot is too close to left wall
    float dErr  = (error - prevError) / max(dt, 1e-3f);
    prevError   = error;

    int delta = constrain(int(Kp*error + Kd*dErr), -SERVO_DEADZONE, SERVO_DEADZONE);
    steeringServo.write(SERVO_CENTER + delta);  // adjust steering within limits
  }
```
- PD control for smooth centering when no emergency
```cpp
  if (encoderTicks >= ENCODER_3LAPS) {
    analogWrite(ENB, 0);                   // stop motor
    steeringServo.write(SERVO_CENTER);     // center steering
    Serial1.println("3 laps (median) reached — stopping.");
    while (true) { /* halt */ }            // end of challenge
  }
```
- Lap counting using the encoder 
Calibration: 10,000 ticks ≈ 30 cm → 1 lap (~2.0 m) ≈ 66,667 ticks
3 laps ≈ ~200,000 ticks. After several trials we set ENCODER_3LAPS to the median value.
 (encoderTicks is updated in the ISR or encoder routine outside this loop.)

## 2. **Obstacle Challenge code**
Import libraries
```python
import cv2
import numpy as np
import serial
import time
from gpiozero import Motor, PWMOutputDevice
```
Handles camera access, image processing, numerical operations, motor control, and serial communication with Arduino.

- Serial connection setup
```python
arduino = serial.Serial("/dev/serial0", baudrate=9600, timeout=1)
time.sleep(2)
```
Establishes a UART connection between the Raspberry Pi and Arduino with a short delay to stabilize communication.

- Motor and speed setup
```python
motor = Motor(forward=20, backward=21)
velocidad = PWMOutputDevice(12)
velocidad.value = 0.85
```
Configures GPIO pins for motor direction and sets PWM speed to 85%.

- Camera setup
```python
cap = cv2.VideoCapture(0)
cv2.namedWindow("Vista Completa", cv2.WINDOW_NORMAL)
cv2.resizeWindow("Vista Completa", 800, 600)
```
Initializes the camera and creates a display window for visual feedback.

- Control and state variables
```python
lineas_detectadas = 0
umbral_linea = 1000
umbral_bloque = 800
linea_detectada = False
girando = False
direccion_giro = None

red_limit_x = 170
green_limit_x = 470
```
Tracks the state of line detection, block turns, and defines positional thresholds to determine when a block has passed.

- Helper function to find mask centroid
```python
def obtener_centro(mask):
    M = cv2.moments(mask)
    if M["m00"] != 0:
        cx = int(M["m10"] / M["m00"])
        return cx
    return None
```
Calculates the X-coordinate of the centroid of a binary mask. Used to know where a block is in the frame.

- Frame capture
```python
ret, frame = cap.read()
if not ret:
    continue
```
Reads the current frame from the camera. If the read fails, the loop skips to the next iteration.

- Block detection (red and green)
```python
roi_bloques = frame[200:320, :]
hsv_bloques = cv2.cvtColor(roi_bloques, cv2.COLOR_BGR2HSV)

# Red mask
lower_red1 = np.array([0, 120, 120])
upper_red1 = np.array([10, 255, 255])
lower_red2 = np.array([160, 120, 120])
upper_red2 = np.array([179, 255, 255])
mask_red = cv2.inRange(hsv_bloques, lower_red1, upper_red1) | cv2.inRange(hsv_bloques, lower_red2, upper_red2)

# Green mask
lower_green = np.array([85, 100, 100])
upper_green = np.array([100, 255, 255])
mask_green = cv2.inRange(hsv_bloques, lower_green, upper_green)

area_rojo = cv2.countNonZero(mask_red)
area_green = cv2.countNonZero(mask_green)
cx_rojo = obtener_centro(mask_red)
cx_green = obtener_centro(mask_green)
```
Detects red and green blocks based on HSV color ranges and calculates their positions and areas.

- Black line detection
```python
roi_nav = frame[160:280, :]
gray = cv2.cvtColor(roi_nav, cv2.COLOR_BGR2GRAY)
_, mask_black = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)

width = mask_black.shape[1]
third = width // 3
left_black = cv2.countNonZero(mask_black[:, 0:third])
center_black = cv2.countNonZero(mask_black[:, third:2*third])
right_black = cv2.countNonZero(mask_black[:, 2*third:3*third])
```
Uses a grayscale region to isolate the black navigation line and counts how many black pixels are in each section.

- Turning and navigation logic
```python
direction = 'C'
```
Initial direction is centered.

If turning based on block previously detected:
```python
if girando:
    if direccion_giro == 'R':
        if cx_rojo is not None and cx_rojo <= red_limit_x:
            girando = False
            direccion_giro = None
        elif right_black > 300:
            direction = 'C'
        else:
            direction = 'R'
    elif direccion_giro == 'L':
        if cx_green is not None and cx_green >= green_limit_x:
            girando = False
            direccion_giro = None
        elif left_black > 300:
            direction = 'C'
        else:
            direction = 'L'
```
If not currently turning:
```python
else:
    if area_rojo > umbral_bloque and cx_rojo is not None:
        direccion_giro = 'R'
        girando = True
        direction = 'R'
    elif area_green > umbral_bloque and cx_green is not None:
        direccion_giro = 'L'
        girando = True
        direction = 'L'
    else:
        min_black = min(left_black, center_black, right_black)
        if min_black == center_black:
            direction = 'C'
        elif min_black == left_black:
            direction = 'L'
        elif min_black == right_black:
            direction = 'R'
```
Controls the robot’s turning behavior based on detected blocks and black line regions.

- Orange line detection for stopping
```python
roi_linea = frame[400:480, :]
hsv = cv2.cvtColor(roi_linea, cv2.COLOR_BGR2HSV)
lower_orange = np.array([10, 150, 150])
upper_orange = np.array([25, 255, 255])
mask_linea = cv2.inRange(hsv, lower_orange, upper_orange)
area_linea = cv2.countNonZero(mask_linea)

if area_linea > umbral_linea and not linea_detectada and lineas_detectadas < 12:
    lineas_detectadas += 1
    linea_detectada = True
    time.sleep(1)

if area_linea <= umbral_linea:
    linea_detectada = False

if lineas_detectadas >= 12:
    direction = 'S'
    motor.stop()
else:
    motor.forward()
```
Counts the number of orange lines the robot crosses. Stops movement after 12 detections.

- Send servo angle to Arduino
```python
if direction == 'L':
    angulo = '58'
elif direction == 'C':
    angulo = '90'
elif direction == 'R':
    angulo = '120'
else:
    angulo = '90'

arduino.write((angulo + "\n").encode())
```
Maps the movement direction to a specific servo angle and sends the command to the Arduino for steering.

## 3. **Servo control code**
- Include libraries
```ino
#include <Servo.h>
#include <math.h>
```
The Servo.h library is included to control servo motors. math.h is included for compatibility but is not directly used in this code.

- Variable and object declarations
```ino
Servo miServo;
int pinServo = 2;
String comando = "";
unsigned long ultimoEnvio = 0;
const unsigned long intervalo = 100;
```
The miServo object is created to control the servo motor. pinServo sets the pin connected to the servo signal wire. The comando variable stores incoming serial commands as a string. ultimoEnvio and intervalo are reserved for timing tasks but are unused here.

- Setup function
```ino
void setup() {
  miServo.attach(pinServo);
  Serial1.begin(9600);
  Serial.begin(9600);
  delay(1000);

  Serial.println("Send angle (0 to 180) via Serial1 to move the servo.");
  miServo.write(90);
}
```
The servo is attached to the specified pin. UART serial communication (Serial1) is started at 9600 baud to talk with the Raspberry Pi, and USB serial (Serial) is started for debugging. The program waits one second for stability. A message with instructions is printed to the debug monitor. The servo is initialized at center position (90 degrees).

- Loop function: reading and processing commands
```ino
void loop() {
  while (Serial1.available() > 0) {
    char c = Serial1.read();

    if (c == '\n' || c == '\r') {
      comando.trim();

      if (comando.length() > 0) {
        int angulo = comando.toInt();

        if (angulo >= 0 && angulo <= 180) {
          miServo.write(angulo);
          Serial.print("Servo moved to ");
          Serial.print(angulo);
          Serial.println(" degrees");
        } else {
          Serial.println("Angle out of range (0–180)");
        }
      }

      comando = "";
    } else {
      comando += c;
    }
  }
}
```
The code reads incoming characters from the UART serial buffer one by one. It accumulates them into a command string until it detects a newline or carriage return character, signaling the end of a command. Then it trims whitespace from the command. If the command is not empty, it converts the string to an integer representing the servo angle. It checks if the angle is between 0 and 180 degrees. If valid, it moves the servo to that angle and prints a confirmation message. If invalid, it prints an error message. After processing, it clears the command buffer to prepare for the next command and continues looping.

## 4. **Run code when the raspberry turns on**
- Import libraries and modules
```python
from gpiozero import Button
from signal import pause
import subprocess
```
The script imports the Button class from gpiozero to handle GPIO pin inputs on the Raspberry Pi. The pause function from the signal module is imported to keep the script running and listening for events. The subprocess module allows the script to run external programs or scripts.

- Initialize button on GPIO pin 16
```python
boton = Button(16)
```
A Button object is created and linked to GPIO pin 16, which is physically connected to a push button on the Raspberry Pi. This button will trigger running the robot’s main program.

- Define function to run the main program
```python
def ejecutar_programa():
    print("Button pressed! Running the main program...")
    subprocess.run(["python3", "/home/diego/WRO_Ingeniero/otracosaahi.py"])
```
This function runs when the button is pressed. It prints a message to the terminal indicating the button press and then uses subprocess.run to execute another Python script (otracosaahi.py), which is the robot’s main control program. The path must be correct and the script must be executable.

- Link button press event to the function
```python
boton.when_pressed = ejecutar_programa
```
The button’s press event is connected to the ejecutar_programa function. When the button is pressed, this function will be called automatically.

- Indicate readiness and keep script running
```python
print("Waiting for the button to be pressed...")
pause()
```
Prints a message to inform the user that the system is ready and listening for the button press. The pause() function keeps the script running indefinitely so it can detect the button press event; without it, the script would exit immediately.

# 🎥 Video demonstration
[![TEKBOT](https://github.com/user-attachments/assets/41548a17-1234-4439-a963-7a1d1d64031e)](https://youtu.be/VGku5C4htFQ?si=9HICP8p2Be4NuSKb)
[![TEKBOT(2)](https://github.com/user-attachments/assets/81b52a5e-d51f-4770-acd4-8bd30d91f259)](https://youtu.be/uQNMZddFvvI?si=1SfpjZH-YJ5O9Mvj)
---
**Stay tuned for updates as we continue to improve our robot's performance and capabilities!**

# References
- 
- https://docs.ultralytics.com/models/yolov8/
- https://colab.research.google.com/github/roboflow/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb
- https://nerdvana.ro/wro-fe/
- https://markdownlivepreview.com/
- https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html

# Shout-Outs

A huge thank you to everyone who made this project possible:

- **The Sutherland Family**  
  For sharing their technical know-how, sourcing crucial parts, and helping with all of our 3D printing needs.

- **Ericka’s Family**  
  For opening up their living room as our track space and pitching in to build the surrounding walls.

- **The Hidalgo Family**  
  For generously donating the printed track mat that serves as our competition surface.

- **Diego’s Family**  
  For welcoming us into their home and providing a comfortable space for our team meetings and build sessions.

We couldn’t have done it without your support—thank you for being an integral part of our journey!

# 📜 License

MIT License

Copyright (c) 2025 Ericka Ceballos

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



