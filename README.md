# TEKBOT WEST's readme <img src="https://upload.wikimedia.org/wikipedia/commons/a/ab/Flag_of_Panama.svg" alt="Bandera de Panamá" width="30"/>
<p align="center">
  <img src="https://github.com/user-attachments/assets/516d8dd3-5f5f-4bb0-b4f9-02e0fee0c09e" alt="TEKBOT (1)">
</p>


[![Instagram](https://img.shields.io/badge/Instagram-%23E9805F.svg?style=for-the-badge&logo=Instagram&logoColor=white)](https://www.instagram.com/tekbot_lab?utm_source=ig_web_button_share_sheet&igsh=ZDNlZDc0MzIxNw==)
[![Facebook](https://img.shields.io/badge/YouTube-%23E4445F.svg?style=for-the-badge&logo=Youtube&logoColor=white)](https://www.youtube.com/@TEKBOT_LAB)


> [!NOTE] 
> This github repository contains many illustrative content such as images,
> gifs and video, so wait till is fully refresh to see it in its entirety.  
> 
# 🐙 Repo's folder structure/overview
```
📦 TEKBOT-WEST-WRO-FE
├── 📁 3D_printables # 3D models ready for printing robot components
├── 📁 MACHINE LEARNING MODEL # contains the trained model for the CORAL accelerator
├── 📁 schemes # Wiring and schematic diagram
├── 📁 src # Source code for robot control and challenge algorithms
├── 📁 team photos # Photos of the team 
├── 📁 vehicle photos # Pictures of the robot at different views
├── 📁 video # Video of testing competition runs
├── 📄 README.md # Main project documentation
```
---

<img width="1000" height="520" alt="AHHHHH_processed_by_imagy" src="https://github.com/user-attachments/assets/4d39ecf9-c464-4f2b-9efc-c3b949ed69be" />
<a href="https://github.com/kieviceb/TEKBOT-WEST-WRO-FE-PANAMA/tree/main/team%20photos">
  <img width="1000" height="2000" alt="BEHIND THE SCREENS (2)" src="https://github.com/user-attachments/assets/34d80203-9c0d-4e8b-a392-d2f8124f5f28" />
</a>


---
# Vehicle Photos 📸
<p align="center">
  <img src="https://github.com/user-attachments/assets/cb52b6d4-e43e-412f-b74b-98dca438176e" alt="TEKBOT (1)">
</p>

<img width="1000" height="1917" alt="Copia de BEHIND THE SCREENS" src="https://github.com/user-attachments/assets/b97cae86-9007-4df4-bbb8-74799222fe50" />


# Components 🧱
A list of all the electrical and mechanical components on the robot.

| <img src="https://github.com/user-attachments/assets/719a51d8-4b14-402d-a462-ba1e4b071c2c" alt="Alt 1" width="200"/> | <img src="https://github.com/user-attachments/assets/3641b928-34c5-4d97-861c-fa08d40c9faa" alt="Alt 1" width="200"/> | <img src="https://github.com/user-attachments/assets/3d5651d3-c8fe-4935-9561-6ca9d9c87a76" alt="Alt 1" width="200"/> | 
| :------------: |:-------------:| :------------:|
|[RASPBERRY PI 5 - 8GB RAM x1](https://a.co/d/7e9fzE2)|[MICROSOFT LIFECAM HD-3000 x1](https://a.co/d/66cNAOh)|[L298N MOTOR DRIVER x1](https://www.steren.com.pa/tarjeta-para-control-de-motores-cc-l298n.html)|
| <img src="https://github.com/user-attachments/assets/5b0ffc5d-ce02-4620-9849-15fdce566702" width="200"/> | <img src="https://github.com/user-attachments/assets/d4170adc-23b9-446f-bac0-1c50b966e00f" alt="Alt 1" width="200"/> | <img src="https://github.com/user-attachments/assets/db07c349-4f9f-4661-8327-3c710a5d41c6" alt="Alt 1" width="200"/> |
|[ARDUINO NANO RP2040 CONNECT x1](https://a.co/d/hI8sZ40) |[FUNDUINO KIT CHASSIS x1](https://a.co/d/fpJSHg1)|[GEEKWORM X1202 4-Cell 5V UPS Shiled+Active Cooler H505 x1](https://a.co/d/hs9rgxH) |
| <img src="https://github.com/user-attachments/assets/8002e73e-910a-462e-b991-5cd5c858e316" width="200"/> |<img src="https://github.com/user-attachments/assets/77157511-58ff-4ee8-9977-d3b53d906af6" alt="Alt 1" width="200"/>| <img src="https://github.com/user-attachments/assets/86d8bf70-708b-426f-896a-9283a74d13df" alt="Alt 1" width="200"/> |
|[JUMPER WIRES](https://a.co/d/cw9IdJk)|[MG996R METAL GEAR SERVO x1](https://a.co/d/cRVAc0u)|[STEREN Li-ion 2800 mAh RRECHARGABLE BATTERY x7](https://www.steren.com.pa/bateria-recargable-li-ion-2800-mah-tipo-18650-1.html)|
| <img src="https://github.com/user-attachments/assets/ff93515e-6ec1-41f5-be4c-ac25b1e24010" width="200"/> |<img src="https://github.com/user-attachments/assets/44a5bca6-2fb6-49b7-8374-93a03705a72b" alt="Alt 1" width="200"/>| <img src="https://github.com/user-attachments/assets/33ee1aaf-4750-4b80-978a-aab5d8d9adeb" alt="Alt 1" width="200"/> |
|[ULTRASONIC DISTANCE SENSOR x3](https://a.co/d/cxbABTR)|[LEGO EV3 DC MOTOR](https://ebay.us/m/6vxrYN)|[COOLING FAN x3](https://a.co/d/frwMkoA)|
| <img src="https://github.com/user-attachments/assets/c4632267-e417-4f33-bfa0-a858d9f1e28e" width="200"/> |<img src="https://github.com/user-attachments/assets/dd5c7b13-b8ad-4aa0-9481-8411c9bf2b1a" alt="Alt 1" width="200"/>| <img src="https://github.com/user-attachments/assets/72f660f8-0134-4bb3-aa97-c8b0ae3c5f5a" alt="Alt 1" width="200"/> |
|[MINI BREADBOARD x1](https://a.co/d/0logMX9)|[PUSH BUTTON x1](https://a.co/d/70cfCgl)|[10K OHM RESISTOR x1](https://a.co/d/dTNRWBD)|

THE MAGIC
| :------------: |
| <img src="https://github.com/user-attachments/assets/be93e47a-3ca9-4626-8b98-f7ea953e2264" alt="Alt 1" width="200"/> |
|[GOOGLE CORAL USB EDGE TPU ACCELARATOR x1](https://a.co/d/eEklwQk)|

# ⚡Circuit diagram

<img width="1920" height="1080" alt="Wiring diagram" src="https://github.com/user-attachments/assets/a8a37846-0950-4cb8-8dc3-2045f952b8c9" />

# Future Engineers Challenge Overview

The **WRO 2025 Future Engineers challenge** challenges teams to design and build a fully autonomous robot capable of navigating a dynamic and randomized racetrack. Using a combination of sensors, computer vision, and advanced control algorithms, the robot must adapt to changing obstacles, follow predefined driving rules, and demonstrate precise maneuvering skills.

### 📌 Competition Format

- **🏁 Open Challenge:**  
  The robot must complete **three (3) laps** around a track where obstacles and track elements are randomly placed, requiring real-time adaptation.

- **🚦 Obstacle Challenge:**  
  The robot must detect and respond correctly to randomly positioned colored markers:  
  - 🟥 **Red signs** → Robot must drive on the right side of the lane.  
  - 🟩 **Green signs** → Robot must drive on the left side of the lane.

  After completing the laps, the robot must find a designated parking zone and perform a precise **parallel parking maneuver** within a confined space, adding an additional technical challenge.

### 📑 Documentation Requirements

Each team is required to maintain a **public GitHub repository** documenting their engineering design process, technical decisions, robot design, and source code. This encourages transparency, collaboration, and learning within the robotics community.

### 🏆 Scoring & Evaluation

Teams are evaluated based on their robot’s **accuracy**, **speed**, and the quality of their **technical documentation**. Points are awarded to teams that effectively balance performance, adaptability to randomized conditions, and innovation in their approach. This challenge promotes not only technical skills in robotics and programming but also problem-solving, teamwork, and creative engineering.
> [!WARNING]
>
> We noticed a translation misunderstanding regarding the rules, at the original english version it says
> that the robot starting within the parking lot is 7 points, but in the spanish version states that the robot should be on
> In the parking lot, not specifying if before or after the challenge is done, but taking as reference the original english version
> the robot starting on the parking lot can score 7 points.
>
<img width="653" height="186" alt="meet the team (3)" src="https://github.com/user-attachments/assets/3d4dfe50-6bb6-4aca-be4a-6e9f3429dde8" />

---

> [!NOTE]
> For detailed rules and regulations (PANAMA ONLY), please refer to the official WRO Panama 2025 Future Engineers document:  
> [WRO 2025 Future Engineers Rules (PDF)](https://fundesteam.nyc3.cdn.digitaloceanspaces.com/FutureEngineers/WRO-2025-FE-Reglas%20Generales.pdf)

<br>


# ⚙️ Mobility Management

### Movement
- **LEGO EV3 DC Motor** driven by an L298N H-bridge on the Arduino Nano RP2040, powered by a 3×18650 Li-ion pack (≈11.1 V) for high torque.
- PWM speed control (pin D3) and direction pins (D4/D5), with automatic halt if HC-SR04 front distance < threshold.
- Receives speed & direction commands over Serial1 from the Raspberry Pi.

### Steering
- **MG996R metal-gear servo** in an Ackermann linkage for accurate, repeatable turns (pin D2).
- Servo angles (20° left, 90° center, 155° right) sent as ASCII strings over Serial1 and output via PWM.

# 🔋 Power Management

Our robot uses a streamlined power system for stable, efficient energy distribution:

- **Geekworm UPS Module**  
  All computing units (Raspberry Pi & Arduino Nano RP2040) are powered directly from the Geekworm Raspberry Pi Wide Input Voltage UPS, providing regulated 5 V output and battery backup—no external power bank needed.

- **Servo Power**  
  The MG996R steering servo draws its 5 V supply from the UPS module, ensuring stable, synchronized operation.

- **Camera and CORAL accelerator Power**  
  The Microsoft LifeCam HD-3000 and the CORAL connect via USB to the Raspberry Pi and is powered through the UPS, eliminating separate power sources.

- **DC Motor Power**  
  The L298N H-bridge and DC motor are driven by a dedicated pack of three 18650 Li-ion cells in series (≈11.1 V), delivering the higher voltage and current the motor requires.

This setup guarantees reliable power under all operating conditions.  


# 📡 Robot Communication

Our robot implements a structured communication system between its two core processing units to ensure synchronized and reliable operation:

- The **Raspberry Pi** (SBC – Single Board Computer) acts as the **master**. It is responsible for high-level tasks such as image processing, object detection, strategy decision-making, and general system coordination.

- The **Arduino Nano RP2040** (SBM – Secondary Board Microcontroller) acts as the **slave**. It is dedicated to real-time control of low-level hardware components, such as driving the servo motor, ultrasonic sensors, LEGO EV3 DC MOTOR and interpreting incoming instructions from the master.

### 🔁 Serial Communication

The two boards communicate via a **UART (Serial) interface** using the following configuration:

- **Raspberry Pi TX (GPIO 14)** → **Arduino RX (pin 1)**  
- **Raspberry Pi RX (GPIO 15)** ← **Arduino TX (pin 0)**

This full-duplex serial link enables:

- The **Raspberry Pi (master)** to send instructions like servo angles or movement triggers based on camera input.
- The **Arduino (slave)** to receive commands and execute precise control over hardware.
- **Optional feedback**: The Arduino can also send status messages or sensor feedback back to the Raspberry Pi when necessary.

This master-slave architecture promotes:

- **Modularity**, by separating high-level logic and low-level control.
- **Responsiveness**, with real-time actions handled by the microcontroller.
- **Scalability**, allowing for future hardware expansions without major rewrites.

The system is optimized for efficiency and robustness in fast-paced robotic challenges.

# 👁️ Sense and Object Detection

Our robot relies on a combination of vision (OpenCV + ML on Coral Accelerator) and ultrasonic sensing to understand its environment and make driving decisions.

## Vision System

<p align="center">
  <img src="https://github.com/user-attachments/assets/5609bb93-1abd-4938-874a-78750336fe9e" alt="Dataset samples" width="350"/>
</p>

- We use the Microsoft LifeCam HD-3000 connected to the Raspberry Pi. The Pi processes the camera feed with OpenCV and a Machine Learning model accelerated by Coral to:
- Detect red and green blocks for obstacle avoidance.
- Recognize the magenta parking spot to initiate the parking routine.
- Detect black walls and estimate their distance.
- Send steering and speed commands to the Arduino Nano RP2040 over Serial.

## Ultrasonic Distance Sensing

<p align="center">
  <img src="https://github.com/user-attachments/assets/6b70fd23-10f5-44be-91a9-fc63dd27e047" alt="Dataset samples" width="350"/>
</p>

- The Arduino Nano RP2040 reads three HC-SR04 ultrasonic sensors (left, center, right) to:
    - Measure the robot’s distance from the walls.
    - Detect obstacles that may not be visible to the camera.
    - Provide redundancy and safety in case of vision failure.
    - This multi-sensor approach ensures robust performance:
    - Vision + ML handles colored block detection, parking, and wall recognition.
    - Ultrasonics provide accurate distance measurements for safety and redundancy.
      
## Machine Learning  

To improve robustness, we trained a **custom object detection model** specifically for our robot’s environment.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/6a6bf5d5-deef-4753-a1c9-7cb3e5085e32" alt="Dataset samples" width="400"/>
</p>

- The dataset was built using **Roboflow** 👉 our data set [here](https://universe.roboflow.com/tekbot-west/tekbot-west-nacional-bbbm9/dataset/2).  
- We collected and labeled images of **green/red blocks, walls, and parking spots** at different **distances, angles, and lighting conditions**, to ensure stable detection during competition with minimal calibration.  

<p align="center">
  <img src="https://github.com/user-attachments/assets/8c8a763d-c597-4432-adf1-6ebd00906060" alt="YOLOv8 Training" width="400"/>
</p>

- The model uses the **YOLOv8 format**, which is well-suited for real-time detection thanks to its **lightweight architecture and speed** — ideal for embedded systems.  
- Training was done in **Google Colab** using GPU acceleration.  
- Finally, we exported the trained model to **TensorFlow Lite**, optimized for the **Coral Edge TPU accelerator** on the Raspberry Pi.  

This pipeline allows the robot to run **real-time detection with low latency**, ensuring reliable navigation and obstacle avoidance.  

# ⚔️ Challenge Strategies
## Open Challenge Strategy 
- **Start Command**  
  When button is start button is pressed on the Raspberry pi sends `'s'` on Serial1 before beginning navigation.

- **Ultrasonic Sensing**  
  Continuously read left, center and right HC-SR04 sensors for obstacle distances.

- **Front Obstacle Handling**  
  If center distance ≤ FRONT_MIN, halt motor; otherwise drive at fixed PWM speed.

- **Emergency Side Avoidance**  
  If left or right distance ≤ SIDE_MIN, immediately steer hard away (servo to 150° or 20°).

- **PD Steering Control**  
  When no emergency, compute error = dR − dL and derivative, apply Kp/Kd to adjust servo within +60° -70° o deadzone around 90°.  
  This ensures smooth alignment between the two side walls.

- **Lap Counting via Encoder**  
  The IMU is no longer used. Instead, we rely on the wheel encoder:  
  - Calibration showed that **10,000 encoder counts ≈ 30 cm of travel**.  
  - Since each lap around the track is ~2 m, that corresponds to ~66,667 encoder counts.  
  - Running multiple trials from different start positions, we computed a **median value** for 3 laps.  
  - The robot stops when the encoder reading reaches this median threshold, ensuring reliable lap counting independent of curves.

- **Completion Condition**  
  Stop motor and center servo once the encoder threshold for 3 laps is reached; remain halted.

- **Serial Feedback**  
  Print encoder values, lap progress, and status messages over Serial1 for real-time monitoring.

## Obstacle Challenge Strategy 
- **Start & Exit Detection**  
  At the beginning, the robot uses **ultrasonic sensors** to measure wall distances and determine **which side it exits from the parking area**.  
  This defines its initial orientation on the track.

- **Machine Learning Object Detection**  
  Instead of simple LAB masking, we use a **YOLOv8-based model** (dataset and labels in Roboflow, trained in Google Colab and exported to TensorFlow Lite, and accelerated with Coral).  
  The model detects **red blocks, green blocks, black walls, and the magenta parking zone** in real time.  

- **Block Evasion Strategy**  
  When a block is detected:  
  - **Red →** compute a temporary target position **to the right side** and steer the robot to bypass the block.  
  - **Green →** compute a temporary target position **to the left side** for evasion.  
  The robot continues toward this evasion target until the block is cleared, then smoothly returns to its central trajectory.  

- **Lap Counting via Parking Detection**  
  Instead of counting curves or encoder ticks, the robot uses **ML detection of the magenta parking spot** as a lap marker.  
  - Each time the robot passes the parking, the counter increases by **1 lap**.  
  - After **3 passes (3 laps)**, the robot transitions to the final parking stage.  

- **PD Steering Control**  
  During normal navigation, the robot still uses **PD control** based on ultrasonic side distances (`dR - dL`) to stay centered in the lane, ensuring smooth driving even without visible blocks.  

- **Final Parking Routine**  
  Once 3 laps are completed, the robot searches for the **magenta parking zone** using ML detection.  
  Upon recognition, it executes a **parallel parking maneuver**, aligning itself precisely inside the designated parking spot.  

- **Serial Communication**  
  All steering (`S<angle>`) and motor commands are sent from the Raspberry Pi to the Arduino Nano via Serial, ensuring synchronized control.

# `</>` Into the codes (code explanations)

## 1. **Open Challenge Code — Main loop (core)**
> To keep this README concise, here we only explain the **main loop** (the core logic).  
> A complete explanation and all helper functions can be found in the [`/src`](./src) folder.  

```cpp
void loop() {

  if (!started) {
    if (Serial1.available() && Serial1.read() == 's') { //button pressed on raspberry pi sends the command to start
      started = true;
      Serial1.println("Starting navigation…");
      prevTime = micros();       // reset PD timing
      prevError = 0;
    } else {
      return;                    // stay idle until 's' is received
    }
  }
```
-  Wait for the start command over Serial1 ('s')
```cpp
 
  float dL = readUltrasonic(TRIG_LEFT,  ECHO_LEFT);   // left wall/obstacle
  float dC = readUltrasonic(TRIG_CENT,  ECHO_CENT);   // front obstacle
  float dR = readUltrasonic(TRIG_RIGHT, ECHO_RIGHT);  // right wall/obstacle

  // C) Front obstacle handling: stop if too close, otherwise run motor at fixed PWM
  if (dC <= FRONT_MIN) {
    analogWrite(ENB, 0);                 // stop motor
  } else {
    analogWrite(ENB, MOTOR_SPEED);       // forward at constant speed
  }
```
- Read distances (cm) from the 3 HC-SR04 ultrasonic sensors
```cpp
  if (dL <= SIDE_MIN_LEFT) {
    steeringServo.write(SERVO_MAX_RIGHT);  // steer right
  }
  else if (dR <= SIDE_MIN_RIGHT) {
    steeringServo.write(SERVO_MAX_LEFT);   // steer left
  }
  else {
```
 - Emergency side avoidance: steer hard away if a wall is too close
```cpp
    unsigned long now = micros();
    float dt = (now - prevTime) / 1e6;     // seconds
    prevTime = now;

    float error = dR - dL;                  // if dR>dL → robot is too close to left wall
    float dErr  = (error - prevError) / max(dt, 1e-3f);
    prevError   = error;

    int delta = constrain(int(Kp*error + Kd*dErr), -SERVO_DEADZONE, SERVO_DEADZONE);
    steeringServo.write(SERVO_CENTER + delta);  // adjust steering within limits
  }
```
- PD control for smooth centering when no emergency
```cpp
  if (encoderTicks >= ENCODER_3LAPS) {
    analogWrite(ENB, 0);                   // stop motor
    steeringServo.write(SERVO_CENTER);     // center steering
    Serial1.println("3 laps (median) reached — stopping.");
    while (true) { /* halt */ }            // end of challenge
  }
```
- Lap counting using the encoder 
Calibration: 10,000 ticks ≈ 30 cm → 1 lap (~2.0 m) ≈ 66,667 ticks
3 laps ≈ ~200,000 ticks. After several trials we set ENCODER_3LAPS to the median value.
 (encoderTicks is updated in the ISR or encoder routine outside this loop.)

## 2. **Obstacle Challenge code**

```python
ok, frame = cap.read()
if not ok:
    continue

Hf, Wf = frame.shape[:2]
y_min  = int(ROI_Y_MIN_RATIO * Hf)
x_l    = int(ROI_X_MARGIN * Wf)
x_r    = int((1.0 - ROI_X_MARGIN) * Wf)
```
read a frame and prepare ROI bands (bottom-focused + side margins)
Grab the camera frame and compute the active ROI: ignore the top band and a small margin on both sides to reduce false triggers and focus on near-field decisions.

```python
rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
inp = cv2.resize(rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)
```
preprocess and run YOLOv8 (EdgeTPU) inference
```python
_interpreter.set_tensor(_input_details[0]['index'], np.expand_dims(inp, 0))
_interpreter.invoke()
out0 = _interpreter.get_tensor(_output_details[0]['index'])
dets = parse_yolov8_tflite(out0, _output_details[0], Wf, Hf, CONF_THRESH, NMS_IOU, _labels)
```
Convert to RGB, resize to model input, invoke TFLite + EdgeTPU, then parse outputs (dequantize, class scores, NMS) into frame-space detections.

```python
valid_L = valid_R = False
has_pklot = False

for d in dets:
    x, y, w, h, name = d['x'], d['y'], d['w'], d['h'], d['name']
    cx, cy = x + w//2, y + h//2

    in_roi = (cy >= y_min) and (x >= x_l) and (x + w <= x_r)
    near   = (cy >= int(NEAR_Y_RATIO * Hf))

    if not in_roi:
        continue

    if name == CLASS_GBLOCK and near:
        valid_L = True    # green → plan left bypass
    elif name == CLASS_RBLOCK and near:
        valid_R = True    # red → plan right bypass
    elif name == CLASS_PKLOT:
        has_pklot = True  # parking marker seen
```
scan detections to set intent flags (valid_L/valid_R) and see PKLOT
For each detection: check ROI and near-field; flag left/right evasion intent from block color; flag PKLOT for lap counting.

```python
now = time.time()
if has_pklot and (now - pklot_last_seen_t) > PKLOT_DEBOUNCE_SEC:
    pklot_last_seen_t = now
    pklot_count += 1
    if (pklot_count >= PKLOT_TARGET_COUNT) and not parking_mode:
        parking_mode = True
        if ser: ser.write(b"PARK\n")
```
debounced PKLOT counting; switch to PARK after N passes
Use a debounce window to avoid multiple counts per pass; after 3 PKLOT sightings, enter PARK mode and notify the Arduino.

```python
cooldown_ok = (now - last_cmd_t) >= COOLDOWN_SEC
streak_L = streak_L + 1 if valid_L else 0
streak_R = streak_R + 1 if valid_R else 0

send_L = cooldown_ok and (streak_L >= TRIGGER_FRAMES) and not parking_mode
send_R = cooldown_ok and (streak_R >= TRIGGER_FRAMES) and not parking_mode

if send_L or send_R:
    cmd = "PATH L" if send_L else "PATH R"
    if ser: ser.write((cmd + "\n").encode("ascii"))
    last_cmd_t = now
    streak_L = streak_R = 0
```
PATH decision: cooldown + streak of valid frames
Require a minimum streak of consecutive frames before acting, and enforce a cooldown so you send only one high-level command (PATH L/PATH R) per window. Suppress PATH when already in PARK mode.

```python
if SEND_STEER_TOWARD_WAYPOINT and aim_points:
    aim_x, aim_y = aim_points[-1]             # latest proposed lateral pass point
    sdeg = steer_towards_x(aim_x, Wf)         # map lateral error to servo [0..180]
    if ser: ser.write(f"S {sdeg}\n".encode("ascii"))
```
(optional) send direct steering toward lateral waypoint
Optionally bias steering directly toward the computed waypoint beside the block; main evasion still uses PATH L/R.

```python
cv2.line(frame, (0, y_min), (Wf, y_min), (255,255,0), 1)
cv2.rectangle(frame, (x_l, 0), (x_r, Hf), (255,255,0), 1)
cv2.putText(frame, f"PKLOT:{pklot_count}  MODE:{'PARK' if parking_mode else 'RUN'}",
            (6, Hf-8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)

cv2.imshow("Coral YOLOv8 -> Serial", frame)
if (cv2.waitKey(1) & 0xFF) == ord('q'):
    break
```
overlays & exit handling
Draw ROI and status overlays; show the preview; quit cleanly on q.

```python
if ser:
    ser.write(b"STOP\n")
    ser.close()
cap.release()
cv2.destroyAllWindows()
```
on exit: stop and cleanup
Send a final STOP, close serial and camera, destroy windows.

## 3. **Servo control code**

- Setup function
```ino
void setup() {
  miServo.attach(pinServo);
  Serial1.begin(9600);
  Serial.begin(9600);
  delay(1000);

  Serial.println("Send angle (0 to 180) via Serial1 to move the servo.");
  miServo.write(90);
}
```
The servo is attached to the specified pin. UART serial communication (Serial1) is started at 9600 baud to talk with the Raspberry Pi, and USB serial (Serial) is started for debugging. The program waits one second for stability. A message with instructions is printed to the debug monitor. The servo is initialized at center position (90 degrees).

- Loop function: reading and processing commands
```ino
void loop() {
  while (Serial1.available() > 0) {
    char c = Serial1.read();

    if (c == '\n' || c == '\r') {
      comando.trim();

      if (comando.length() > 0) {
        int angulo = comando.toInt();

        if (angulo >= 0 && angulo <= 180) {
          miServo.write(angulo);
          Serial.print("Servo moved to ");
          Serial.print(angulo);
          Serial.println(" degrees");
        } else {
          Serial.println("Angle out of range (0–180)");
        }
      }

      comando = "";
    } else {
      comando += c;
    }
  }
}
```
The code reads incoming characters from the UART serial buffer one by one. It accumulates them into a command string until it detects a newline or carriage return character, signaling the end of a command. Then it trims whitespace from the command. If the command is not empty, it converts the string to an integer representing the servo angle. It checks if the angle is between 0 and 180 degrees. If valid, it moves the servo to that angle and prints a confirmation message. If invalid, it prints an error message. After processing, it clears the command buffer to prepare for the next command and continues looping.

## 4. **Run code when the raspberry turns on**
- Import libraries and modules
```python
from gpiozero import Button
from signal import pause
import subprocess
```
The script imports the Button class from gpiozero to handle GPIO pin inputs on the Raspberry Pi. The pause function from the signal module is imported to keep the script running and listening for events. The subprocess module allows the script to run external programs or scripts.

- Initialize button on GPIO pin 16
```python
boton = Button(16)
```
A Button object is created and linked to GPIO pin 16, which is physically connected to a push button on the Raspberry Pi. This button will trigger running the robot’s main program.

- Define function to run the main program
```python
def ejecutar_programa():
    print("Button pressed! Running the main program...")
    subprocess.run(["python3", "/home/frankie/WRO_Ingeniero/execute.py"])
```
This function runs when the button is pressed. It prints a message to the terminal indicating the button press and then uses subprocess.run to execute another Python script (otracosaahi.py), which is the robot’s main control program. The path must be correct and the script must be executable.

- Link button press event to the function
```python
boton.when_pressed = ejecutar_programa
```
The button’s press event is connected to the ejecutar_programa function. When the button is pressed, this function will be called automatically.

- Indicate readiness and keep script running
```python
print("Waiting for the button to be pressed...")
pause()
```
Prints a message to inform the user that the system is ready and listening for the button press. The pause() function keeps the script running indefinitely so it can detect the button press event; without it, the script would exit immediately.

# 🎥 Video demonstration
[![TEKBOT](https://github.com/user-attachments/assets/41548a17-1234-4439-a963-7a1d1d64031e)](https://youtu.be/VGku5C4htFQ?si=9HICP8p2Be4NuSKb)
[![TEKBOT(2)](https://github.com/user-attachments/assets/81b52a5e-d51f-4770-acd4-8bd30d91f259)](https://youtu.be/uQNMZddFvvI?si=1SfpjZH-YJ5O9Mvj)
---
**Stay tuned for updates as we continue to improve our robot's performance and capabilities!**

# References
- https://docs.ultralytics.com/models/yolov8/
- https://colab.research.google.com/github/roboflow/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb
- https://nerdvana.ro/wro-fe/
- https://markdownlivepreview.com/
- https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html

# Shout-Outs

A huge thank you to everyone who made this project possible:

- **The Sutherland Family**  
  For sharing their technical know-how, sourcing crucial parts, and helping with all of our 3D printing needs.

- **Ericka’s Family**  
  For opening up their living room as our track space and pitching in to build the surrounding walls.

- **The Hidalgo Family**  
  For generously donating the printed track mat that serves as our competition surface.

- **Diego’s Family**  
  For welcoming us into their home and providing a comfortable space for our team meetings and build sessions.

We couldn’t have done it without your support—thank you for being an integral part of our journey!

# 📜 License

MIT License

Copyright (c) 2025 Ericka Ceballos

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



